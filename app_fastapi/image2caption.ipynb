{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 241/241 [00:00<00:00, 51.1kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 798kB [00:00, 1.48MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 456kB [00:00, 1.17MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 1.36MB [00:00, 1.94MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 120/120 [00:00<00:00, 27.5kB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 228/228 [00:00<00:00, 47.2kB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m image_to_text \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mimage-to-text\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m image_to_text(\u001b[39m\"\u001b[39m\u001b[39mhttps://ankur3107.github.io/assets/images/image-captioning-example.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/__init__.py:849\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39m# Instantiate feature_extractor if needed\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_extractor, (\u001b[39mstr\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 849\u001b[0m     feature_extractor \u001b[39m=\u001b[39m AutoFeatureExtractor\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    850\u001b[0m         feature_extractor, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    851\u001b[0m     )\n\u001b[1;32m    853\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    854\u001b[0m         feature_extractor\u001b[39m.\u001b[39m_processor_class\n\u001b[1;32m    855\u001b[0m         \u001b[39mand\u001b[39;00m feature_extractor\u001b[39m.\u001b[39m_processor_class\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mWithLM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    856\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model_name, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    857\u001b[0m     ):\n\u001b[1;32m    858\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/huggingface/lib/python3.9/site-packages/transformers/models/auto/feature_extraction_auto.py:347\u001b[0m, in \u001b[0;36mAutoFeatureExtractor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         feature_extractor_class \u001b[39m=\u001b[39m feature_extractor_class_from_name(feature_extractor_class)\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mreturn\u001b[39;00m feature_extractor_class\u001b[39m.\u001b[39;49mfrom_dict(config_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    348\u001b[0m \u001b[39m# Last try: we use the FEATURE_EXTRACTOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m FEATURE_EXTRACTOR_MAPPING:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "image_to_text(\"https://ankur3107.github.io/assets/images/image-captioning-example.png\")\n",
    "\n",
    "# [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ViTImageProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "ViTImageProcessor.from_pretrained('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m VisionEncoderDecoderModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m feature_extractor \u001b[39m=\u001b[39m ViTImageProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "        i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_step(['doctor.e16ba4e4.jpg'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
